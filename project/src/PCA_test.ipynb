{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Analysis\n",
    "\n",
    "In this document, we will try to get a dimensionality reduction using PCA and get a feature extraction from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "First we load up data that is relevant to the songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import RDD\n",
    "from pyspark import SparkContext\n",
    "\n",
    "rdd_analysis_songs = sc.pickleFile(\"../data/analysis-songs\")\n",
    "rdd_musicbrainz_songs = sc.pickleFile(\"../data/musicbrainz-songs\")\n",
    "rdd_metadata_songs = sc.pickleFile(\"../data/metadata-songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all relevant features into one RDD\n",
    "\n",
    "We have features relevant to songs dispersed over 3 RDDs. We need to join them based on the key to get a vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.lib.recfunctions as rfn\n",
    "\n",
    "def merge_array_parameters(row):\n",
    "    return (row[0], rfn.merge_arrays([row[1][0], row[1][1]], flatten=True))\n",
    "\n",
    "big_rdd = rdd_analysis_songs.join(rdd_metadata_songs).map(merge_array_parameters)\n",
    "big_rdd = big_rdd.join(rdd_musicbrainz_songs).map(merge_array_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see all the feature names that are available that are directly linked to songs. We will not be using the segments data for simplification. Some features in the `analysis-songs` dataset are compiled using these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('analysis_sample_rate',\n",
       " 'audio_md5',\n",
       " 'danceability',\n",
       " 'duration',\n",
       " 'end_of_fade_in',\n",
       " 'energy',\n",
       " 'idx_bars_confidence',\n",
       " 'idx_bars_start',\n",
       " 'idx_beats_confidence',\n",
       " 'idx_beats_start',\n",
       " 'idx_sections_confidence',\n",
       " 'idx_sections_start',\n",
       " 'idx_segments_confidence',\n",
       " 'idx_segments_loudness_max',\n",
       " 'idx_segments_loudness_max_time',\n",
       " 'idx_segments_loudness_start',\n",
       " 'idx_segments_pitches',\n",
       " 'idx_segments_start',\n",
       " 'idx_segments_timbre',\n",
       " 'idx_tatums_confidence',\n",
       " 'idx_tatums_start',\n",
       " 'key',\n",
       " 'key_confidence',\n",
       " 'loudness',\n",
       " 'mode',\n",
       " 'mode_confidence',\n",
       " 'start_of_fade_out',\n",
       " 'tempo',\n",
       " 'time_signature',\n",
       " 'time_signature_confidence',\n",
       " 'track_id',\n",
       " 'analyzer_version',\n",
       " 'artist_7digitalid',\n",
       " 'artist_familiarity',\n",
       " 'artist_hotttnesss',\n",
       " 'artist_id',\n",
       " 'artist_latitude',\n",
       " 'artist_location',\n",
       " 'artist_longitude',\n",
       " 'artist_mbid',\n",
       " 'artist_name',\n",
       " 'artist_playmeid',\n",
       " 'genre',\n",
       " 'idx_artist_terms',\n",
       " 'idx_similar_artists',\n",
       " 'release',\n",
       " 'release_7digitalid',\n",
       " 'song_hotttnesss',\n",
       " 'song_id',\n",
       " 'title',\n",
       " 'track_7digitalid',\n",
       " 'idx_artist_mbtags',\n",
       " 'year')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_rdd.first()[1].dtype.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize features\n",
    "\n",
    "The next step is to vectorize the data into vectors that can be used for primary component analysis. We are going to ignore any features that are not numeric, these are either invalid (`genre` is always an empty string) or not necessary (e.g. `track_id` is represented by `track_7digitalid`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.functions import isnan\n",
    "import math\n",
    "\n",
    "def map_features_to_vectors(input_row):\n",
    "    song_id = input_row[0]\n",
    "    feature_vec = input_row[1]\n",
    "    \n",
    "    # take only number values\n",
    "    row_data = [feature_vec[x][0] for x in feature_vec.dtype.names if np.isreal(feature_vec[x][0]) and not x.endswith(\"hotttnesss\")]\n",
    "    # replace nan by 0\n",
    "    row_data = map(lambda x: 0 if math.isnan(x) else x, row_data)\n",
    "    \n",
    "    label = feature_vec['song_hotttnesss'][0]\n",
    "    \n",
    "    return (Vectors.dense(row_data), float(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sqlContext.createDataFrame(big_rdd.map(map_features_to_vectors), [\"features\", \"labels\"])\n",
    "data = data.filter(isnan(data.labels) == False).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_ml = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca_ml.fit(data)\n",
    "transformed = model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2\n",
      "k=4\n",
      "k=6\n",
      "k=8\n",
      "k=10\n",
      "k=12\n",
      "k=14\n",
      "k=16\n",
      "k=18\n",
      "k=20\n",
      "k=22\n",
      "k=24\n",
      "k=26\n",
      "k=28\n",
      "k=30\n",
      "k=32\n",
      "k=34\n",
      "k=36\n",
      "k=38\n",
      "k=40\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "results = []\n",
    "\n",
    "for k in range(2, data.first().features.size+1, 2):\n",
    "    print \"k=%d\" % k\n",
    "    \n",
    "    dtrain, dtest = data.map(lambda x: LabeledPoint(x.labels, x.features)).randomSplit([0.8, 0.2])\n",
    "    \n",
    "    pca_ml = PCA(k=k, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    pca = pca_ml.fit(data)\n",
    "    \n",
    "    model_pca = LinearRegressionWithSGD.train(dtest)\n",
    "    \n",
    "    valuesAndPreds = dtest.map(lambda p: (p.label, model_pca.predict(p.features)))\n",
    "    MSE = valuesAndPreds \\\n",
    "        .map(lambda vp: (vp[0] - vp[1])**2) \\\n",
    "        .mean()\n",
    "    results.append(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myspark]",
   "language": "python",
   "name": "conda-env-myspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
