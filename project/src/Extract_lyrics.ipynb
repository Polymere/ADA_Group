{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract lyrics data from dataset\n",
    "This notebook was created to extract formatted RDDs form the musiXmatch dataset on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/\"\n",
    "file_path_train = \"mxm_dataset_train.txt\"\n",
    "file_path_test = \"mxm_dataset_test.txt\"\n",
    "\n",
    "rdd = sc.textFile(folder_path + file_path_train)\n",
    "rdd2 = sc.textFile(folder_path + file_path_test)\n",
    "#rdd.saveAsPickleFile(os.path.join(\"output/\", re.sub('\\\\.', '-', file_path)), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the list of word mappings from the line starting with `%i`. We keep the first element (`%i`) as the indices start at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordMappings = rdd.filter(lambda x: isinstance(x, basestring) and x.startswith(\"%\")).first().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterComments(rdd):\n",
    "    \"\"\"\n",
    "    :param rdd: RDD on which to perform the operations\n",
    "    :return: An RDD where lines that start with '#' or '%' are filtered\n",
    "    \"\"\"\n",
    "    return rdd.filter(lambda x: isinstance(x, basestring) and not x.startswith(\"#\") and not x.startswith(\"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapToPairs(rdd):\n",
    "    \"\"\"\n",
    "    :param rdd: RDD on which to perform the operations\n",
    "    :return: An RDD where the lines as strings have been transformed to \n",
    "    \"\"\"\n",
    "    def mapFn(x):\n",
    "        fragments = x.split(\",\")\n",
    "        return (fragments[0], {\"TrackID\": fragments[1], \"words\": fragments[2:]})\n",
    "    return rdd.map(mapFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapWords(wm, rdd):\n",
    "    \"\"\"\n",
    "    :param rdd: RDD on which to perform the operations\n",
    "    :return: An RDD where the words list has been transformed to include the name of each word instead of its ID.\n",
    "    \"\"\"\n",
    "    def processLine(l):\n",
    "        words = l[1][\"words\"]\n",
    "        \n",
    "        newWordsList = []\n",
    "        \n",
    "        for wl in words:\n",
    "            frags = wl.split(\":\")\n",
    "            wordID = int(frags[0])\n",
    "            wordCount = int(frags[1])\n",
    "            \n",
    "            if (wordID >= 0 and wordID <= 50):\n",
    "                newWordsList.append((wm[wordID], wordCount))\n",
    "        \n",
    "        l[1][\"words\"] = newWordsList\n",
    "        return l\n",
    "    \n",
    "    return rdd.map(processLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processingPipeline(rdd):\n",
    "    \"\"\"\n",
    "    :param rdd: RDD on which to perform the operations\n",
    "    :return: An RDD on which the above operations have all been performed in one step\n",
    "    \"\"\"\n",
    "    rdd = filterComments(rdd)\n",
    "    rdd = mapToPairs(rdd)\n",
    "    rdd = mapWords(wordMappings, rdd)\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_out = processingPipeline(rdd)\n",
    "rdd2_out = processingPipeline(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of lines in the train dataset is 210519\n",
      "The amount of lines in the test dataset is 27143\n"
     ]
    }
   ],
   "source": [
    "print(\"The amount of lines in the train dataset is %d\" % rdd_out.count())\n",
    "print(\"The amount of lines in the test dataset is %d\" % rdd2_out.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_out.saveAsPickleFile(os.path.join(\"output/\", re.sub('\\\\.', '-', file_path_train)), 1000)\n",
    "rdd2_out.saveAsPickleFile(os.path.join(\"output/\", re.sub('\\\\.', '-', file_path_test)), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myspark]",
   "language": "python",
   "name": "conda-env-myspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
