{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract lyrics data from dataset\n",
    "This notebook was created to extract formatted RDDs form the musiXmatch dataset on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/\"\n",
    "file_path_train = \"mxm_dataset_train.txt\"\n",
    "file_path_test = \"mxm_dataset_test.txt\"\n",
    "# only used for output\n",
    "file_path_merged = \"mxm_dataset_all.txt\"\n",
    "\n",
    "rdd = sc.textFile(folder_path + file_path_train)\n",
    "rdd2 = sc.textFile(folder_path + file_path_test)\n",
    "#rdd.saveAsPickleFile(os.path.join(\"output/\", re.sub('\\\\.', '-', file_path)), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the list of word mappings from the line starting with `%i`. \n",
    "**The indices start at 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordMappings = rdd.filter(lambda x: isinstance(x, basestring) and x.startswith(\"%\")).first()[1:].split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a few processing steps that allow for easier manipulation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterComments(rdd):\n",
    "    \"\"\"\n",
    "    :param rdd: RDD on which to perform the operations\n",
    "    :return: An RDD where lines that start with '#' or '%' are filtered\n",
    "    \"\"\"\n",
    "    return rdd.filter(lambda x: isinstance(x, basestring) and not x.startswith(\"#\") and not x.startswith(\"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapToPairs(rdd):\n",
    "    \"\"\"\n",
    "    :param rdd: RDD on which to perform the operations\n",
    "    :return: An RDD where the lines as strings have been transformed to \n",
    "    \"\"\"\n",
    "    def mapFn(x):\n",
    "        fragments = x.split(\",\")\n",
    "        return (fragments[0], {\"TrackID\": fragments[1], \"wordsID\": fragments[2:]})\n",
    "    return rdd.map(mapFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapWords(wm, rdd):\n",
    "    \"\"\"\n",
    "    :param rdd: RDD on which to perform the operations\n",
    "    :return: An RDD where the words list has been transformed to include the name of each word instead of its ID.\n",
    "    \"\"\"\n",
    "    def processLine(l):\n",
    "        words = l[1][\"wordsID\"]\n",
    "        \n",
    "        newWordsList = []\n",
    "        \n",
    "        for wl in words:\n",
    "            frags = wl.split(\":\")\n",
    "            wordID = int(frags[0])\n",
    "            wordCount = int(frags[1])\n",
    "            \n",
    "            word = wm[wordID-1]\n",
    "            newWordsList.append((word, wordCount))\n",
    "        \n",
    "        l[1][\"words\"] = newWordsList\n",
    "        return l\n",
    "    \n",
    "    return rdd.map(processLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processingPipeline(rdd, wm):\n",
    "    \"\"\"\n",
    "    :param rdd: RDD on which to perform the operations\n",
    "    :return: An RDD on which the above operations have all been performed in one step\n",
    "    \"\"\"\n",
    "    rdd = filterComments(rdd)\n",
    "    rdd = mapToPairs(rdd)\n",
    "    rdd = mapWords(wm, rdd)\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_out = processingPipeline(rdd, wordMappings)\n",
    "rdd2_out = processingPipeline(rdd2, wordMappings)\n",
    "rdd_merged = rdd_out.union(rdd2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of lines in the train dataset is 210519\n",
      "The amount of lines in the test dataset is 27143\n",
      "The amount of lines in the whole dataset is 237662\n"
     ]
    }
   ],
   "source": [
    "print(\"The amount of lines in the train dataset is %d\" % rdd_out.count())\n",
    "print(\"The amount of lines in the test dataset is %d\" % rdd2_out.count())\n",
    "print(\"The amount of lines in the whole dataset is %d\" % rdd_merged.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'TRAAAAV128F421A322',\n",
       " {'TrackID': u'4623710',\n",
       "  'words': [(u'i', 6),\n",
       "   (u'the', 4),\n",
       "   (u'you', 2),\n",
       "   (u'to', 2),\n",
       "   (u'and', 5),\n",
       "   (u'a', 3),\n",
       "   (u'me', 1),\n",
       "   (u'it', 1),\n",
       "   (u'my', 1),\n",
       "   (u'is', 2),\n",
       "   (u'of', 3),\n",
       "   (u'your', 1),\n",
       "   (u'that', 1),\n",
       "   (u'are', 2),\n",
       "   (u'we', 2),\n",
       "   (u'am', 2),\n",
       "   (u'will', 2),\n",
       "   (u'for', 4),\n",
       "   (u'be', 1),\n",
       "   (u'have', 2),\n",
       "   (u'so', 1),\n",
       "   (u'this', 1),\n",
       "   (u'like', 2),\n",
       "   (u'de', 1),\n",
       "   (u'up', 1),\n",
       "   (u'was', 2),\n",
       "   (u'if', 1),\n",
       "   (u'got', 1),\n",
       "   (u'would', 1),\n",
       "   (u'been', 1),\n",
       "   (u'these', 2),\n",
       "   (u'seem', 1),\n",
       "   (u'someon', 1),\n",
       "   (u'understand', 1),\n",
       "   (u'pass', 1),\n",
       "   (u'river', 1),\n",
       "   (u'met', 1),\n",
       "   (u'piec', 1),\n",
       "   (u'damn', 1),\n",
       "   (u'worth', 1),\n",
       "   (u'flesh', 1),\n",
       "   (u'grace', 1),\n",
       "   (u'poor', 2),\n",
       "   (u'somehow', 1),\n",
       "   (u'ignor', 1),\n",
       "   (u'passion', 1),\n",
       "   (u'tide', 1),\n",
       "   (u'season', 1),\n",
       "   (u'seed', 1),\n",
       "   (u'resist', 1),\n",
       "   (u'order', 2),\n",
       "   (u'piti', 1),\n",
       "   (u'fashion', 1),\n",
       "   (u'grant', 1),\n",
       "   (u'captur', 2),\n",
       "   (u'ici', 1),\n",
       "   (u'soil', 1),\n",
       "   (u'patienc', 1),\n",
       "   (u'social', 2),\n",
       "   (u'highest', 2),\n",
       "   (u'slice', 1),\n",
       "   (u'leaf', 1),\n",
       "   (u'lifeless', 1),\n",
       "   (u'arrang', 1),\n",
       "   (u'wilder', 1),\n",
       "   (u'shark', 1),\n",
       "   (u'devast', 1),\n",
       "   (u'element', 1)],\n",
       "  'wordsID': [u'1:6',\n",
       "   u'2:4',\n",
       "   u'3:2',\n",
       "   u'4:2',\n",
       "   u'5:5',\n",
       "   u'6:3',\n",
       "   u'7:1',\n",
       "   u'8:1',\n",
       "   u'11:1',\n",
       "   u'12:2',\n",
       "   u'13:3',\n",
       "   u'14:1',\n",
       "   u'15:1',\n",
       "   u'18:2',\n",
       "   u'19:2',\n",
       "   u'20:2',\n",
       "   u'21:2',\n",
       "   u'23:4',\n",
       "   u'25:1',\n",
       "   u'26:2',\n",
       "   u'28:1',\n",
       "   u'30:1',\n",
       "   u'36:2',\n",
       "   u'42:1',\n",
       "   u'45:1',\n",
       "   u'54:2',\n",
       "   u'56:1',\n",
       "   u'57:1',\n",
       "   u'68:1',\n",
       "   u'99:1',\n",
       "   u'192:2',\n",
       "   u'249:1',\n",
       "   u'264:1',\n",
       "   u'356:1',\n",
       "   u'389:1',\n",
       "   u'561:1',\n",
       "   u'639:1',\n",
       "   u'656:1',\n",
       "   u'687:1',\n",
       "   u'761:1',\n",
       "   u'773:1',\n",
       "   u'804:1',\n",
       "   u'869:2',\n",
       "   u'914:1',\n",
       "   u'1035:1',\n",
       "   u'1156:1',\n",
       "   u'1221:1',\n",
       "   u'1287:1',\n",
       "   u'1364:1',\n",
       "   u'1407:1',\n",
       "   u'1533:2',\n",
       "   u'1857:1',\n",
       "   u'2096:1',\n",
       "   u'2117:1',\n",
       "   u'2482:2',\n",
       "   u'2548:1',\n",
       "   u'2705:1',\n",
       "   u'2723:1',\n",
       "   u'2868:2',\n",
       "   u'2992:2',\n",
       "   u'3455:1',\n",
       "   u'3717:1',\n",
       "   u'3851:1',\n",
       "   u'4322:1',\n",
       "   u'4382:1',\n",
       "   u'4613:1',\n",
       "   u'4713:1',\n",
       "   u'4906:1']})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data sample\n",
    "rdd_merged.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_out.saveAsPickleFile(os.path.join(\"output/\", re.sub('\\\\.', '-', file_path_train)), 1000)\n",
    "rdd2_out.saveAsPickleFile(os.path.join(\"output/\", re.sub('\\\\.', '-', file_path_test)), 1000)\n",
    "rdd_merged.saveAsPickleFile(os.path.join(\"output/\", re.sub('\\\\.', '-', file_path_merged)), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myspark]",
   "language": "python",
   "name": "conda-env-myspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
