{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRICS_VECTOR_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxm_data = sc.pickleFile(\"../data/mxm_dataset_all/\")\n",
    "mbz_data = sc.pickleFile(\"../data/musicbrainz-songs/\").map(lambda x: (x[0], {'year': x[1]['year'][0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenDicts(ls):\n",
    "    if len(ls) > 1:\n",
    "        acc = ls[0].copy()\n",
    "        for x in ls[1:]:\n",
    "            acc.update(x)\n",
    "        return acc\n",
    "    else:\n",
    "        return ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = mxm_data.join(mbz_data).map(lambda x: (x[0], flattenDicts(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggWordCount(localRes, newElem):\n",
    "    if len(localRes) == 0:\n",
    "        res = {}\n",
    "        for (k, v) in newElem[1]:\n",
    "            res[k] = v\n",
    "        return res\n",
    "    else:\n",
    "        for (k, v) in newElem[1]:\n",
    "            if k in localRes:\n",
    "                localRes[k] += v\n",
    "            else:\n",
    "                localRes[k] = v\n",
    "        return localRes\n",
    "    \n",
    "def combResults(a, b):\n",
    "    if len(a) == 0:\n",
    "        return b\n",
    "    else:\n",
    "        res = {}\n",
    "        for k in a:\n",
    "            res[k] = a[k]\n",
    "        for k in b:\n",
    "            if k in res:\n",
    "                res[k] += b[k]\n",
    "            else:\n",
    "                res[k] = b[k]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = rdd.map(lambda x: (x[0], x[1][\"words\"])).aggregate({}, aggWordCount, combResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list is taken from the sklearn stop_words package\n",
    "# https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py\n",
    "# which in turn is taken from the Glasgow Information Retrieval Group \n",
    "# http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n",
    "ENGLISH_STOP_WORDS = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "\"yourselves\"]\n",
    "\n",
    "stemmed_stop_words = ['onli', 'whi', 'somethi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_occ_totals(occurences):\n",
    "    acc = {}\n",
    "\n",
    "    for k in occurences:\n",
    "        if k not in stemmed_stop_words and k not in ENGLISH_STOP_WORDS:\n",
    "            acc[k] = occurences[k]\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cnts = filter_occ_totals(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love : 3280\n",
      "know : 2999\n",
      "like : 2669\n",
      "just : 2445\n",
      "come : 2216\n",
      "oh : 2037\n",
      "got : 2034\n",
      "time : 1794\n",
      "que : 1721\n",
      "let : 1718\n",
      "say : 1613\n",
      "babi : 1594\n",
      "want : 1566\n",
      "make : 1527\n",
      "yeah : 1447\n",
      "way : 1349\n",
      "feel : 1349\n",
      "ca : 1346\n",
      "la : 1317\n",
      "caus : 1249\n"
     ]
    }
   ],
   "source": [
    "sorted_keys = sorted(filtered_cnts, key=filtered_cnts.get, reverse=True)\n",
    "\n",
    "for k in sorted_keys[:20]:\n",
    "    print(\"%s : %d\" % (k, filtered_cnts[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_represented_keys = sorted_keys[:LYRICS_VECTOR_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_song_to_vector(x):\n",
    "    vector = []\n",
    "    \n",
    "    for k in most_represented_keys:\n",
    "        found = False\n",
    "        for w, occ in x[1][\"words\"]:\n",
    "            if k == w:\n",
    "                vector.append(occ)\n",
    "                found = True\n",
    "                break;\n",
    "                \n",
    "        if not found:\n",
    "            vector.append(0)\n",
    "            \n",
    "    return vector\n",
    "\n",
    "vectorized_rdd = rdd.map(map_song_to_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myspark]",
   "language": "python",
   "name": "conda-env-myspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
